<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Yu Group - Research</title>
<style type="text/css">
.Description {
	font-family: Georgia, "Times New Roman", Times, serif;
	font-size: small;
	color: #666;
	text-align: left;
}
.Names2 {
	font-family: Georgia, "Times New Roman", Times, serif;
	font-size: small;
	font-weight:bold;
	color: #666;
	text-align: left;
	vertical-align:text-top;
}
.Names {
	font-family: Georgia, "Times New Roman", Times, serif;
	font-size: medium;
	color: #666;
	text-align: left;
}
.Titles {
	font-family: Georgia, "Times New Roman", Times, serif;
	font-size: large;
	font-weight:bold;
	color: #666;
	text-align: left;
}
a:link {
	color: #900;
}
a:visited {
	color: #900;
}
a:hover {
	color: #333;
}
</style>
</head>

<body>
<table width="automatic" border="0">
  <tr>
    <td width="50">&nbsp;</td>
    <td width="50">&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td width="50">&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Titles"><a href="#ares">Research Areas</a></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2" class="Names">Our research spans many applied and theoretical problems in statistical machine learning, <a href="#info">Information Theory</a>, <a href="#rand">random matrices</a>, <a href="#net">Network Tomography</a>, and <a href="#remo">Remote Sensing</a>. Many of our applied problems involve image processing, atmospheric science, <a href="#sens">Sensor Networks</a>, <a href="#entro">neuroscience</a>, <a href="#japa">Natural Language Processing</a>, and <a href="#finan">financial markets</a>. Although these are very different disciplines, many statistical problems recur, such as <a href="#anis">regularization</a>, <a href="#model">model selection</a>, matrix decomposition, and computational problems such as exploration and modeling of data with extremely high dimensionality and numbers of observations. </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3"><span class="Titles">Current Projects</span></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2" class="Description">Anisotropic Ridge Regression for Prediction</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Covariance Estimation in Finance</td>
    <td>&nbsp;</td>
  </tr>
		  <tr>
		    <td>&nbsp;</td>
		    <td>&nbsp;</td>
		    <td class="Description" colspan="2">Efficient Computation for Eigen Problems of Large Random Matrices</td>
		    <td>&nbsp;</td>
		  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Entropy Estimation for Neural Spike Trains</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Financial Market State Classification and Prediction</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Japanese Text Transliteration</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Distributed Data Compression in Sensor Networks</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
	<td>&nbsp;</td>
	<td>&nbsp;</td>
    <td class="Description" colspan="2">Natural Image-Visual Cortex fMRI</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3"><span class="Titles">Past Projects</span></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Titles"><a name="ares" id="ares">Research Areas</a></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
      <td>&nbsp;</td>
      <td colspan="3">&nbsp;</td>
      <td>&nbsp;</td>
  </tr>
  <tr>
      <td>&nbsp;</td>
      <td colspan="3" class="Names"><a name="info">Information Theory and Minimum Description Length</a></td>
      <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td width="50" class="Names2">
    <p>Overview</p></td>
    <td width="automatic" class="Description"><p>The Minimum Description Length (MDL) approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed interest within the statistics community. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework (link to Barron, Rissannen and Yu, 1998, and Hansen and Yu, 2001) discriminates between competing model classes based on the complexity of each description based on a model class. Precisely, the MDL Principle states that</p>
<p>Choose the model that gives the shortest description of data.</p>
<p>The complexity of a description is measured by the code length for the data based on the model.</p>
  <p>  Yu's group's another interest has been Audio Compression. Recently Gerald Schuller, Bin Yu, Dawei Huang, and Bern Edler developed a coding method which achieves leading compression ratios and a low lag for a wide variety of audio sources. Bin's group's work was focused on using prediction method to reduce redundancy which has strong connection to boosting, competitive on-line statistics and MDL.</p></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">M. Hansen and <strong>B. Yu</strong> (2002). <a href="http://www.stat.berkeley.edu/~yugroup/downloads/mdl_glm.ps" target="_top">Minimum Description Length Model Selection Criteria for Generalized Linear Models</a>.  Tech. Report 619, Statistics Dept, UC Berkeley.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">M. Hansen and B. Yu (2000). <a href="http://www.stat.berkeley.edu/~binyu/ps/lmdl.ps" target="_top">Wavelet thresholding via MDL for natural images</a>. IEEE Trans. Inform. Theory (Special Issue on Information Theoretic Imaging). vol. 46, 1778-1788.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">R. Jornsten, W. Wang, B. Yu, and K. Ramchandran (2002). <a href="http://www.stat.berkeley.edu/~binyu/ps/spj.ps" target="_top">Microarray image compression: SLOCO and the effects of information loss</a>. Signal Processing Journal (Special Issue on Genomic Signal Processing). Tech. Report 620, Statistics Dept, UC Berkeley.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">Gerald Schuller, B. Yu, Dawei Huang, and Bern Edler (2002). <a href="http://www.stat.berkeley.edu/~binyu/ps/coding.ps" target="_top">Perceptual Audio Coding using Pre- and Poster- Filters and Lossless Compression</a>. IEEE Trans. Speech and Audio Processing. Vol. 10 (6), 379-390</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td> 
    <td colspan="3" class="Names"><a name="model">Model Selection</a></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>BLASSO - Boosted LASSO</p>
      <p>CAP - Composite Absolute Penalties</p></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2" class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="net">Network Tomography</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>The Internet has evolved from a small tightly controlled network serving only a few users in the late 1970's to the immense decentralized multi-layered collection of heterogeneous terminals, routers and other platforms that we encounter today when surfing the web. The lack of centralized control has allowed Internet service providers (ISP)'s to develop a rich variety of user-services at different quality-of-service (QoS) levels. However, in such a decentralized environment quantitative assessment of network performance is difficult. One cannot depend on the cooperation of individual servers and routers to freely transmit vital network statistics such as traffic rates, link delays, and dropped packet rates. Indeed, an ISP may regard such information as highly confidential. On the other hand, sophisticated methods of active probing and/or passive monitoring can be used to extract useful statistical quantities that can reveal hidden network structure and detect and isolate congestion, routing faults, and anomalous traffic. The problem of extracting such hidden information from active or passive traffic measurements falls in the realm of statistical inverse problems; an area which has long been of interest to signal.</p>
        A fundamental ingredient in the successful design, control and management of coming networks will be the accurate measurement and characterization of its dynamics. For the task of network management and monitoring, the computation cost is high, and currently Yu's group has developped a pseudo likelihood approach to cut down the computational cost. Also we view the data network project for a constrained independent component analysis (CICA) point of view. Our proposed research focuses on the following goals:
          <ul><li>Development of rigorous statistical estimation methodology for multicast internal link distribution through end-to-end measurements.</li>
          <li>Develop and evaluate parametric and non-parametric estimators for link-level performance metrics such as loss rate, delay statistics.</li>
          <li>Multicast network topology identification.</li>
          <li>Network origin-destination (OD) matrix inference.</li>
      </ul></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">Gang Liang, Bin Yu.</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">R. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu. (2003) <a href="http://www.stat.berkeley.edu/~binyu/ps/cny.pdf" target="_top">Internet Tomography (Recent Developments)</a>, Statistical Science (invited).							   </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">G. Liang and B. Yu. (2003) <a href="http://www.stat.berkeley.edu/~binyu/ps/pseudo-ieee.ps" target="_top">Maximum Pseudo Likelihood Estimation in Network Tomography</a>, IEEE Transaction on Signal Processing (special issue on data network).
</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">M. Coates, A. Hero, R. Nowak and B. Yu. (2002) <a href="http://www.stat.berkeley.edu/~binyu/ps/large-scale-inference.ps" target="_top">Large scale inference and tomography for network monitoring and diagnosis</a>, Signal Processing Magazine.</td>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description"></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="entro">Entropy Estimation</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description">Shannon's seminal work on InformationTheory made popular the theoretical quantity known as the entropy
	
				    H := -\sum_{x\in\mathcal{X}} P(x) \log P(x)

				of an discrete probability distribution $P$ over a possibly infinite
				space $\mathcal{X}$.  The basic statistical problem is its estimation from data being conceived as random variables $X_1, \ldots, X_n$ with $X_i$ distributed according to $P$, but $P$ unknown.  An apparent method of
				estimating the entropy is to apply the formula after estimating $P(x)$ for all
				$x \in \mathcal{X}$, but estimating a discrete probability distribution is, in
				general, a difficult nonparametric problem.</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Application in Neuroscience</p></td>
    <td class="Description">The problem of "neural coding'' is to elucidate the representation and
				transformation of information in the nervous system. \cite{Perkel:1968} An
				appealing way to attack neural coding is to take the otherwise vague notion of
				"information'' to be defined in Shannon's sense, in terms of entropy.
				\cite{Shannon:1948} This project began in the early days of cybernetics
				\cite{Wiener:1948} \cite{MacKay:1952}, received considerable impetus from work
				summarized in the book \emph{Spikes: Exploring the Neural Code}
				\cite{Rieke:1997}, and continues to be advanced by many investigators. In most
				of this research, the findings concern the mutual information between a
				stimulus and a neuronal spike train response. For a succinct overview see
				\cite{Borst:1999}. The mutual information, however, is the difference of
				marginal and expected conditional entropies.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">Vince Vu, Bin Yu</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2" class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="rand">Efficient Computation for Eigen Problems of Large Random Matrices</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>Data often comes in the form of a matrix, and random matrix theory offers a probabilistic look at such objects. We aim to exploit, and perhaps supplement, the results of random matrix theory in the view of data analysis. One simple example of this is PrincipleComponentAnalysis (PCA). Here the goal is to find a lower dimensional representation of the data matrix that captures a substantial proportion of the variability. PCA considers linear subspaces based on an eigenanalysis of the data matrix. In this context, random matrix theory provides insight and guidance about the eigenvalues of "typical" data matrices.</p>
<p>In modern statistics random matrices are in presence almost everywhere, especially in cases involves large amount of data, such as image, video, text, micro array, etc. All these kinds of data have two common features. First, the number of variables is large, often increases linearly as the number of observations, where the classical condition for consistency is violated; second, each single entry of the data matrix is a random variable, with a certain vector distribution (in either columns or rows). Traditional treatments on such data matrix include Singular Value Decomposition (SVD) and covariance matrix estimation, where in both cases the eigenvalues and eigenvectors of both sample covariance matrix and population covariance matrix play an important role.</p>

<p>However, under the context of large random matrices, the eigenvalues and eigenvectors of sample covariance matrix would be hard to retrieve, because of the noise effect and the computation. One example can show the noise effect in particular: in the i.i.d normal case, when p, the number of variables, grows linearly with n, the number of observations, the eigenvalues of sample covariance matrix will be spread on a certain interval while the population covariance matrix has p identical eigenvalues. On the other hand, it is also well known the computation cost for SVD is n * p^2, for n>p and n^2 * p for n</p>

<p>Recent advances in both random matrix theory and matrix computation provide some promising tools and ideas for the problems described above. The random matrix theory reveals the relationship between the sample covariance matrix and population covariance matrix in terms of there Empirical Spectral Distributions (the empirical distribution of all eigenvalues), which is useful to retrieve information about eigenvalues of the population covariance matrix from the sample covariance matrix. At the same time, the theory subset sampling and low rank approximation makes it possible to compute the eigenvalues as well as eigenvectors of large matrix in an efficient way. The goal for this project is to find out how can we combine these two tools to efficiently retrieve eigenstructures of the population covariance matrix from the sample covariance matrix.</p>
</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">Jing Lei, Bin Yu</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="remo">Classification in Remote Sensing</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>Multiangle Imaging Spectro Radiometor (MISR) is a sensor aboard the Terra satellite and part of the Earth Observation System (EOS) program. With the multi-angle information provided by MISR, we are trying to classify the cloud and the ice/snow covered land at both poles. ( Here are one MISR image and the classification result from our approach, Linear Correlation Matching Classification.)</p>
      <p>The MISR instrument offers a new and promising avenue for cloud detection methods. MISR has nine cameras looking at the Earth from different angles simultaneously. The nine camera angles are 70.5O, 60O, 45.6O, 26.1O forward, nadir and 26.1O, 45.6O, 60O, 70.5O afterward Each angle has four observing bands (red, green, blue, and near-infrared). MISR covers the earth every 16 days. ( Watch an illustration for the MISR instrument here.) The high dimensionality and huge size of the data provide statisticians huge space to play around.</p></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">Shi, T., Yu, B., and Braverman, A. (2002). MISR cloud/ice classification using linear correlation matching. Technical Report #630, Department of Statistics, U.C. Berkeley.
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">Shi, T., Yu, B., and Braverman, A. (2002). Discriminating cloud over ice/snow using MISR data. Invited Talk in American Geophysical Union (AGU) 2002 Fall Meeting at San Francisco, CA</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names">&nbsp;</td>
    <td class="Description">Shi, T., Yu, B., and Braverman, A. (2002) Discriminating Cloud over Ice/Snow using MISR data. Poster in MISR science team meeting at Pasadena, CA. [Poster]</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2" class="Names">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="anis">Anisotropic Ridge Regression for Prediction</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description">The classical linear model states that the response variable $Y$ is a linear function of the $p$-dimensional covariate vector $X$, with noise added, i.e. $Y = X\beta + \epsilon$, where $\beta$ is a $p$-dimensional vector of coefficients and $\epsilon$ mean 0, variance $\sigma^$ noise.  Given data $(Y_1, X_1), \ldots, (Y_n, X_n)$, classical regression provides a framework for making inference about $\beta$, conditional on the observerved covariates $X$.</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="finan">Covariance Estimation in Finance</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>Estimation of the covariance matrix of asset returns is one of the most important problems in finance.  In particular, the theory of mean/variance optimization for portfolio allocation assumes an accurate estimate of the covariance matrix.  Due to the large number of assets available today and the fact that the market is not stationary for long time periods, there is not nearly enough data to rely on classical theories.</p>
<p>By combining modern regularization techniques with subject matter information we expect to get a more accurate view of the risks involved in investment problems.</p></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">Nate Coehlo, Bin Yu</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="japa">Japanese Text Transliteration</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description"><p>Working with data provided by Microsoft, we are working on new statistical exploration and methods for improving ranking of transliteration candidates. The specific application is called [[http://www.microsoft.com/windows/ie/ie6/downloads/recommended/ime/default.mspx][IME - Input Method Editor]]. As the feature space of this data involves nearly 1 million dimensions, analysis and modeling require new approaches for visualization, efficient selection of features, and development of ranking models.</p>
<p>
		As of March 2007, we have applied Boosted Lasso (BLasso) to the problem, and this approach outperforms all other published approaches to language modeling in this context.  Further work, yet to be published, has produced gains equal to or better than BLasso's improvement over the previous best method.  In addition, we have developed a few heuristics for exploring this high dimensional data and for identifying levels of data quality.</p></td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">David Purdy, Bin Yu</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"><a name="sens">Sensor Networks</a></td>
    <td>&nbsp;</td>
  </tr>
      <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Overview</p></td>
    <td class="Description">Wireless sensor networks are formed by a set of small devices that, in addition to its sensing equipment, are furnished with a radio for wireless communication and a small processor. 
						They are expected to sense the environment cheaply and non-intrusively.
						It has been used in applications varying from collecting data on habitat environment in the Great Duck Island in Maine to performing measurements aimed at Structure Health Monitoring on the Golden Gate Bridge in California. 
						One key advantage of these sensing devices is the reduced need for maintenance. 
						Ideally, the sensing nodes are deployed once and keep sensing the environment without direct human intervention for long periods of time.

						Energy is the scarcest resource on the network.
						Since communications is one of the most energy intensive tasks, sending each measurement to a central station may greatly reduce the lifetime of the network. 
						Statistics can be employed to make the data collection more efficiently.
						One such way consists in using statistical modeling to summarize the data of each node in the network individually.
						Once the parameters of model are fit to the data, they can be transmitted instead of the complete series of measurements.
						Model selection methods are important here as the complexity of the model can be used to meet precision requirements, bandwidth and power availability.
						Additional energy savings can be made by exploring any spatial structure existing between the data collected by neighboring nodes. </td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Projects</p></td>
    <td class="Description">Guilherme Rocha, Bin Yu</td>
    <td>&nbsp;</td>
  </tr>
    <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"><p>Papers</p></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td class="Names2"></td>
    <td class="Description">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td colspan="3" class="Names"></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td colspan="2">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</table>
<p class="Description">&nbsp;</p>
</body>
</html>