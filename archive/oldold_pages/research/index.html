<html>

<head>
<title>Yu Group</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen" />
</head>

<body>
<div id="page">

<div id="header">
	<h1><a href="">Yu Group</a></h1>
</div>

<div id="navmenu">
    <div id="navmenutext">
        <ul>
            <li><a href="../">home</a></li>
            <li><a href="../people/">people</a></li>
            <li><a href="../research/">research</a></li>
            <li><a href="../publications/">publications</a></li>
            <li><a href="../software/">software</a></li>
        </ul>
    </div>
</div>


<!-------------------Begin-Content---------------->
<div id="content">

<h1>Research</h1>
<hr size="1" noshade />

<h3>Japanese text transliteration</h3>
<p>
Working with data provided by Microsoft, we are working
on new statistical exploration and methods for improving ranking of
transliteration candidates.  The specific application is called IME -
Input Method Editor.  As the feature space of this data involves
nearly 1 million dimensions, analysis and modeling require new
approaches for visualization, efficient selection of features, and
development of ranking models.
</p>
<p>
People Involved: David Purdy, Bin Yu, Martin Wainwright
</p>

<h3>Efficient Computation for Eigen Problems of Large Random Matrices</h3>
<p>
In modern statistics random matrices are in presence almost everywhere, especially in cases
involves large amount of data, such as image, video, text, micro array, etc. All these kinds of
data have two common features. First, the number of variables is large, often increases linearly
as the number of observations, where the classical condition for consistency is violated; second,
each single entry of the data matrix is a random variable, with a certain vector distribution (in
either columns or rows). Traditional treatments on such data matrix include Singular Value
Decomposition (SVD) and covariance matrix estimation, where in both cases the eigenvalues and
eigenvectors of both sample covariance matrix and population covariance matrix play an important
role.
</p>
<p>
However, under the context of large random matrices, the eigenvalues and eigenvectors of sample
covariance matrix would be hard to retrieve, because of the noise effect and the computation. One
example can show the noise effect in particular: in the i.i.d normal case, when p, the number of
variables, grows linearly with n, the number of observations, the eigenvalues of sample
covariance matrix will be spread on a certain interval while the population covariance matrix has
p identical eigenvalues. On the other hand, it is also well known the computation cost for SVD is
n * p^2, for n>p and n^2 * p for n<p, which is prohibitive when n and p are both large.
</p>
<p>
Recent advances in both random matrix theory and matrix computation provide some promising tools
and ideas for the problems described above. The random matrix theory reveals the relationship
between the sample covariance matrix and population covariance matrix in terms of there Empirical
Spectral Distributions (the empirical distribution of all eigenvalues), which is useful to
retrieve information about eigenvalues of the population covariance matrix from the sample
covariance matrix. At the same time, the theory subset sampling and low rank approximation makes
it possible to compute the eigenvalues as well as eigenvectors of large matrix in an efficient
way. The goal for this project is to find out how can we combine these two tools to efficiently
retrieve eigenstructures of the population covariance matrix from the sample covariance matrix.
</p>
<p>
People Involved: Jing Lei, Bin Yu
</p>
<!-------------------End-Content---------------->


</div>
</body>

</html>
